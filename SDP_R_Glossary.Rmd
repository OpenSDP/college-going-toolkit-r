---
title: "SDP R Glossary"
author: "Strategic Data Project"
date: "Center for Education Policy Research at Harvard University"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
    toc_depth: 2
    includes:
      in_header: harvardheader.tex
      before_body: harvard_prefix.tex
  html_document: default
---


# 1. Getting Started with R and RStudio

## Setting up your R Environment

First, download and install R for your platform. Then download and install the 
RStudio Integrated Development Environment (IDE). RStudio 
is a free and open source IDE that is available on all computing platforms. 
RStudio provides a number of features for working with R 
including code completion, code highlighting, and tools to speed up literate 
(combining R code, R output, text, and figures in one document) programming in 
the R environment. 

Consult the Quickstart guide for more details about getting started with R. 

## External Packages

Unlike other statistical software, much of the power of R lies in user contributed 
packages that you can install to add functionality to the base R system. In R 
you load packages by using the `library()` function. You can install packages 
from CRAN, a respository of user-produced packages, using the built in 
`install.packages()` function. You can update installed packages using the 
`update.packages()` function.

```{r libraries, eval=FALSE}
library(tidyverse)
install.packages("tidyverse")
update.packages()
```

R has some example data built in which will be used in the examples below. You can 
load the `mtcars` dataset with the command below:

```{r include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```


```{r setup}
library(tidyverse)
library(magrittr)
# Let's load an example dataset
data(mtcars)
```

## Getting Help

In R you can get help at any time by simply typing `?` and the command of 
of interest. For example `? summary` will open up the help page for the 
summary function.

If you are not sure of the exact name of a function you can use `??` to search 
through the help files available in your R installation. For example `?? model` 
will search for all files that reference the term model. 

## Inserting Comments

In R the `#` character is used to denote a comment. It is useful to insert 
frequent comments that describe what your code is doing. 

Anything the follows a `#` character will be ignored by R until the next 
linebreak. To make a block of comments use the following:

```{r comments}
# Commenting one line

### Starting a block of comments
### The spacing makes it clear
### Being consistent is key
```

## Breaking Up Long Lines of Code

If a code is long, it can be practical to break it into multiple lines. 
In R there is no need to specifically tell R that code continues on the 
next line. If you use the RStudio IDE it will assist you in making appropriate 
linebreaks. 

## Quotes

R recognizes both `'single'` and `"double"` quotes. At times it may also be 
necessary to use both types of quotes together, such as when passing a string 
that contains a command that operates on a string. 

```{r pasteyQuotes, eval=FALSE}
eval(parse(text=("summary(mtcars['Valiant', ])")))
```

# 2. Data Management

## Handling Datasets

Working directories are the number one source of confusion for new R users. A 
working directory is where R will first look for files and folders. You can 
find files and folers beyond this directory by using relative paths. 

If you use RStudio and RStudio Projects you can eliminate a lot of confusion 
and hassle with setting and changing working directories. Create a new project 
in RStudio and it will always open in its own folder with the working directory 
set to that folder. 

To view and set the working directory use:

`getwd()` to view the current working directory
`setwd("C:\MyProject")` to set the working directory to "MyProject"

To remove an object from the R workspace:

`rm(object)` 

R can read a variety of data files. R can store files in a native file format 
using either the `.rda` or `.RData` extension. To use files of this type: 

`load('mydata.rda')`

To read in data in other formats you will likely want an external library. For 
data from other statistical software, the `haven` package is excellent at 
reading and writing these files and has the greatest compatibility. 

```{r stataExample, eval=FALSE}
library(haven)
mydata <- read_stata(file = "MyStataFile.dta") 
```

For flat text files like a `.csv` or `.tsv` or fixed-width files use `readr`. 

```{r readrExample, eval=FALSE}
library(readr)
mydata <- read_csv(file = "MyCSVfile.csv")
```

Note that when reading in data you must tell R what name you will assign this 
data in the workspace. This is because R allows you to read in multiple data 
files simultaneously. Try to use a short and meaningful name for your data object.

R can read Excel files directly through a variety of packages (`readxl`), 
but if possible it is usually preferred to use Excel to convert these to CSV first.

You can also load data directly from a remote data source. R can connect to 
many external data stores directly. ODBC connections are very common. To connect 
to an ODBC connection

```{r RODBCexample, eval=FALSE}
library(RODBC)
channel <- odbcConnect(dsn = "mydatasource", 
                       uid="mylogin", pwd="mypassword")

dat1 <- sqlQuery(channel, "SELECT COLUMN1, COLUMN2, COLUMN3
                            FROM SCHEMA.TABLE")
```

Combining datasets in R is easy. You can load multiple datasets into the workspace 
simultaneously and combine them in a variety of ways using common values between 
them. 

The simplest syntax for merging comes from the `dplyr` package which uses a 
command structure similar to SQL. 

- `inner_join` does a 1 to 1 merge 
- `full_join` does a many to many merge
- `left_join` does a one to many merge
- `right_join` does a many to one merge



## Dataframes

A dataset is represented in R, most commonly, as a dataframe. Dataframes are the 
tabular representation of data you read in. A dataframe is a rectangular representation 
where data types must be consistent within columns, but can vary between columns. 

R has a number of very powerful features for working with single dataframes. The 
ability to have multiple dataframes in memory simultaneously a source of much of 
the difference between R and other statistical programming languages. 

When working with dataframes you need to understand a few key functions and 
operators. `$` is an operator that allows you to access elements of a dataframe. 
Whenever you want to access a specific column by name you can use the `$` operator. 
Here is an example:

```{r}
mtcars$hp
```

The `$` teslls R that to access the `hp` column in the `mtcars` dataframe. 
This can get a bit cumbersome at times, for example when you want to access 
data conditionally:

```{r}
mtcars$hp[mtcars$carb > 2]
```

In this case the `[]` serves as an index to the full `mtcars$hp` object. Even 
though you want to index the `mtcars$hp` object, you still need to tell R 
that the index you want to use is from `mtcars`. The flexibility of allowing 
indexes from anywhere comes at the cost of more verbose code. In the toolkit and 
later in this glossary there are some techniques using external packages that 
can help in the case of working with a single dataframe. 

Three more essential functions to quickly understanding your data are:

```{r}
str(mtcars)
```

This function shows the names of variables, dimensions of the dataset, the type 
of variable each variable is, and the first few values.

```{r}
summary(mtcars)
```

This function provides a column-wise summary of each variable in the dataset. 

```{r}
head(mtcars)
```

This function lets us look at the top few (or bottom few if you use `tail()`) 
rows of the data. You can also use the `View()` function to interactively browse 
the data in RStudio.

## Structuring Datasets

Collapsing data in R is achieved best by using the `group_by` and `summarize` 
commands in the `dplyr` package. This requires being explicit about how you 
want to summarize each field from many observations into one. 

```{r collapse}
library(dplyr); library(magrittr)

# Let's calculate horsepower by number of cylinders
collapsedData <- mtcars %>% group_by(cyl) %>% 
  summarize(meanHP = mean(hp))

collapsedData
```

Alternatively, if your variables can all be summarized by the same function 
you can do the following:

```{r collapseAll}
collapsedData <- mtcars %>% select(cyl, hp, disp, wt, drat, qsec) %>% 
  group_by(cyl) %>% 
  summarize_all(.funs = "mean")

collapsedData
```

Any variables not specified will be dropped, but you can retain the original 
data and just create a new object, e.g. `collapsedData`, for your summarized 
data. 

### Reshaping

Reshaping data in R can follow two main approaches. If you need to reshape an 
entire dataset, the base R `reshape` command is powerful and flexible. Read 
the documentation carefully. 

```{r}
stocks <- data.frame(
  time = as.Date('2009-01-01') + 0:9,
  X = rnorm(10, 0, 1),
  Y = rnorm(10, 0, 2),
  Z = rnorm(10, 0, 4)
)

stocksLong <- reshape(stocks, idvar = "time", 
                      varying = list(2:4),
                      direction = "long", v.names = "price", 
                      times = c("X", "Y", "Z"), timevar = "stock", 
                      new.row.names = 1:30)
head(stocksLong)
```

The flexibility of the `reshape` command makes it hard sometimes to understand. 
Consult the examples in the documentation by using `?reshape` if necessary. 

If you need to expand or contract a few variables by an identifier column, the 
`tidyr` package provides an easy to remember syntax and interface to quickly 
do this. 

```{r}
library(tidyr)

stocks <- data.frame(
  time = as.Date('2009-01-01') + 0:9,
  X = rnorm(10, 0, 1),
  Y = rnorm(10, 0, 2),
  Z = rnorm(10, 0, 4)
)
head(stocks)
```

To reshape this in R using `tidyr`, there is a simple vocabulary of either gathering 
some variables together, or spreading some variables out. Here there are three 
columns that are really a variable (the stock name), and the value in each 
column is really a variable (price). In general R, strongly prefers **long** format 
data. 


```{r}
stocksm <- stocks %>% gather(stock, price, -time)
head(stocksm)
```

Once you have gathered the data together to represent time, price, and stock name 
you can easily reshape it to meet our needs:

```{r}
stocksm %>% spread(stock, price) %>% head
```

```{r}
stocksm %>% spread(time, price) %>% head

```

This declarative style of writing code is easier to read in the future.

## Handling variables


In R there is an important distinction between variables contained within a 
`data.frame` and variables stored in the global environment. 

Variables stored in a `data.frame` are accessed by any of the following methods:

```{r, eval=FALSE}
mtcars$hp
mtcars[, "hp"]
mtcars[, 1]
```

### Dropping variables

To drop variables you can do any of the following

```{r, eval=FALSE}
mtcars$hp <- NULL
mtcars[, "hp"] <- NULL
mtcars <- mtcars[, -4]

# dplyr
library(dplyr); library(magrittr)
dataframe %<>% select(-variable)

```

### Filtering data

In R, there is no specific keep command. You can pass a numeric index of column 
positions, or pass column names to drop any variables you do not want. 

```{r}
names(mtcars) # read column names
```

```{r, eval=FALSE}
mtcars <- mtcars[, c(1:3, 8)]
# equivalent to:
mtcars <- mtcars[, c("mpg", "cyl", "disp", "vs")]
```

Using `dplyr` and the `select` function there are a number of powerful ways 
to specify which variables you want to keep. 

```{r, eval=FALSE}

mtcars %<>% select(starts_with("mpg"))
mtcars %<>% select(ends_with("s"))
mtcars %<>% select(contains("hp"))
mtcars %<>% select(matches("*cyl"))
mtcars %<>% select(one_of(names(mtcars)))
mtcars %<>% select(everything())

```

### Creating variables

Variable creation in R is as simple as assigning a variable in a data.frame a 
value.

```{r}
mtcars$mpgPerCyl <- NA # creates an empty vector with missing values
mtcars$mpgPerCyl <- 4 # creates an empty vector and assigns it a constant

# If your replacement is not length 1 or length of the vector, it will not 
# work
# mtcars$mpgPerCyl <- c(1, 5, 8) 

```

If you want to assign a created variable some value based on values of other 
variables in the data:

```{r, eval=FALSE}
mtcars$mpgBinary <- NA
mtcars$mpgBinary[mtcars$mpg > 25] <- 1
mtcars$mpgBinary[mtcars$mpg <= 25] <- 0

## Alternatively you can use the ifelse function for simple assignments

mtcars$mpgBinary <- ifelse(mtcars$mpg > 25, 1, 0)

```

Generating new variables as functions of other variables is also simple:

```{r}
mtcars$mpgMean <- mean(mtcars$mpg) 
# the single mean(mpg) will be repeated the length of mtcars$mpg

```

Generating a new variable that is a function of multiple other variables is also 
simple

```{r, eval=FALSE}
mtcars$mpgPerCyl <- mtcars$mpg / mtcars$cyl # creates vector with values right away
mtcars$gearsAndCarbs <- mtcars$gear + mtcars$carb

# Be careful of order of operations
mtcars$ComplexVar <- mtcars$qsec^2 / (mtcars$disp - mtcars$hp)
```

You can also generate an empty `data.frame` as well. This is useful for setting 
up a data structure you will later fill with values from other calculations. This 
can save memory and enforce good code by declaring your expectations about the 
results of calculations. 

```{r}
mydata <- data.frame(var1 = NA, var2 = NA)

```

### Replacing variables

In R, replacing variables is as easy as overwriting them:

```{r, eval=FALSE}
mtcars$ComplexVar <- mtcars$qsec^2 / (2 * mtcars$disp - mtcars$hp)
```

### Handling duplicates


R comes with the built in `duplicated` function which returns a logical vector 
the same length as the number of rows, TRUE if the row is duplicated. The built 
in dataset `iris` has an example of this:

```{r}
duplicated(iris)
# If you only want a summary
table(duplicated(iris))

```

By default, `duplicated` checks for duplication across *all* columns in the 
data. To specify the columns of interest, just select the variables before 
the duplicated call

```{r}
# Duplicates across two columns
table(duplicated(iris[, 1:2]))
# Duplicates across first three columns, fewer
table(duplicated(iris[, 1:3]))
```

To drop duplicated rows the results of the `duplicated` call can be passed as a 
row index to the `data.frame` itself:

```{r}
nrow(iris)
iris <- iris[duplicated(iris), ]
nrow(iris)
```

This is a bit awkward, so another way to do it is:

```{r}
iris %<>% distinct(.keep_all = TRUE)
#.keep_all tells R you want to return all columns, not just those that are distinct
```

## Variable types

In R there are four common variable types:

- numeric
- integer
- character
- factor

To test what type a variable is you use the `is.type` construction, or you can 
ask R to return the class of the variable directly:

```{r}
is.numeric(mtcars$hp)
is.integer(mtcars$cyl)
class(mtcars$carb)
```

To convert between types, you use the `as.type` construction, or you can 
set the class of the variable directly:

```{r}
class(mtcars$hp)
mtcars$hp <- as.integer(mtcars$hp)
class(mtcars$hp) <- "integer"
class(mtcars$hp)
```

Note that the `as.type` functions are not particularly savvy. For example, 
`as.numeric("1,000,000")` will return `NA`. There are a number of packages that 
implement special parsers for dealing with these types of variables, for example, 
the `decomma` function in the `eeptools` package handles the above case.

## Summarizing variables

R has convenient built in summaries for the base data types:

```{r}
summary(mtcars$carb)
summary(mtcars)
summary(iris)

```

These return a 5 number summary of numeric variables and other useful information. 

Another summary method that is helpful is the `str` command, which provides a 
look at the first values occurring in a data.frame or variable, as well as the 
attributes of the object.

```{r}
str(iris$Species)
str(iris)

```

You can also view the data using the `View` command:

```{r eval=FALSE}
View(mtcars)
View(mtcars[mtcars$hp = 110, ])

```

### Tabulating variables

In R the `table` command is useful for understanding the distribution of values 
a variable takes. You can make 1-way to n-way tables easily. Table works best 
for character, factor, and integer values. If you use a numeric with a high 
number of unique values, `table` will not be helpful.

```{r}
table(iris$species)
```

If you want to look at the distribution of a numeric variable it is best to use 
the `summary` function, or for a more detailed look, the `quantile` function.

Tabulating two variables is as simple as passing them both as arguments to the 
`table` function:

```{r}
iris$SepalWidthInt <- round(iris$Sepal.Width, digits = 0)
table(iris$SepalWidthInt, iris$Species)
```

Note that the `table` command ignores missing values by default. If your vector 
has missing values then the sum of counts returned by the function will be shorter 
than the length of the vector by the number of elements that take the `NA` value.

```{r}
ProblematicVector <- c("A", "B", "C", "A", "B", "D", NA, NA, NA, NA, "A", "C", "C", 
                       "C", "C", "C")
N <- length(ProblematicVector)
table(ProblematicVector)
sum(table(ProblematicVector))
N
```

Keep this in mind when counting proportions and percentages belonging to categories. 

### Counting variables

To just count the number of values in a vector, use `length`. 

```{r}
length(mtcars$cyl)
length(mtcars$hp[mtcars$hp > 140])
table(mtcars$hp > 140)
```

Note that for an entire dataset, `length` is defined as the number of **columns**. 


```{r}
length(mtcars)
# use nrow to get the length of individual vectors
nrow(mtcars)
```


To look at only the unique values in a dataset use the `unique` 
function. This returns a vector of all unique values of a vector. It works on 
all variable types.


```{r}
unique(mtcars$carb)

```

If unique is combined combine this with length, you can count the number of 
distinct values a variable has:

```{r}
length(unique(mtcars$carb))
```

## Organizing Variables

There are two approaches to reordering rows in a dataset in R. The first approach uses 
indexing:

```{r eval=FALSE}
mtcars <- mtcars[order(mtcars$disp, mtcars$cyl), ]

# To reverse the order
mtcars <- mtcars[order(-mtcars$disp, -mtcars$cyl), ]

```

If the variables you include in the `order` statement are not integers or 
numeric, R will attempt to use the underlying integer representation of the 
factor variable. If they are character, it will default to alphabetic.

Another way to do this is using the `arrange` command from dplyr. 

```{r}
mtcars <- arrange(mtcars, cyl, disp)
# The %<>% operator saves some typing
mtcars %<>% arrange(cyl, disp)

# use desc() to order descending
mtcars <- arrange(mtcars, desc(cyl))
mtcars %<>% arrange(desc(cyl))
```

A common task is to do tasks group-wise to your data where group is defined as 
variable in the dataset. In R this is best achieved using the `dplyr` package 
and using the `group_by` command:

```{r}
mtcars %<>% group_by(gear) %>% 
  mutate(meanHPbyGear = mean(hp))

mtcars %<>% group_by(vs, am) %>% 
  mutate(countVSAM = n())
```

In this case the `mutate` function allows us to append a variable to the ungrouped 
dataset that will be repeated with the same value for each group. If the 
`summarize` function is used instead, the dataframe would be collapsed to one 
row per group.


### Ordering variables

The order variables appear in a dataset is most easily controlled by using their 
numeric index. 

```{r}
names(mtcars)
mtcars <- mtcars[, c(1, 3, 2, 5:9, 4, 10, 11)]
# Anything not indexed will be dropped!
names(mtcars)
```

You can also order variables using the `select` function in `dplyr`:

```{r eval=FALSE}
mtcars %<>% select(mpg, cyl, disp, hp, drat, wt)
# Anything not selected will be dropped
```

### Renaming variables

Variable names are just a character value that is stored in the `names` object. 
For example, `names(mydata)` returns a character vector the same length of the 
number of columns of the dataset. To rename variables replace specific 
elements in that vector. 

```{r}
names(mtcars)[7] <- "engconfig"
names(mtcars)
```


To transform multiple names you can do the following:

```{r eval=FALSE}
names(iris) <- tolower(names(iris)) # lowercase
names(iris) <- toupper(names(iris)) # uppercase
names(iris) <- chartr("m", "M", names(iris)) # capitalize all m values

# prefix
names(mtcars)[3:8] <- paste("prefix", names(mtcars)[3:8], sep = "_")
# suffix
names(mtcars)[3:8] <- paste(names(mtcars)[3:8], "suffix", sep = "_")
# substitute character pattern with another pattern in names
# here replace "zzz", with empty ""
names(mtcars) <- gsub("zzz", "", names(mtcars))
```

### Labeling values

R does not have extensive codebook or variable labeling functionality. Most of 
the time if you want labeled values you want to use a `factor` variable in R. 
Factors can ordered or unordered. 

### Listing unique values

To look at the distinct or unique values of a variable, use the `unique` command.

## Using system variables

R does not have system variables, but you can create global variables to keep 
track of attributes of the objects in your work environment. If you want to 
store an integer that represents the number of rows in your data at a point in 
time you can use the `nrow` command and create a new variable `N` to store that 
information. However, if you add or remove rows from `mydata` this will not be 
updated. 

```{r}
N <- nrow(mydata)
N
```

R does have environment variables, but these are special variables that tell R 
how to connect with other programs or to behave in a particular mode. 

# 3. Functions

In R you can find functions in the base R package, in add-on "packages" of code 
that you load into your workspace with `library(mypackage)` or that you define in 
a script and read directly into memory. 

```{r functions}
# create a function that counts distinct values of a variable
nvals <- function(x){
  length(unique(x))
}

nvals(mtcars$carb)

```

Functions are objects in the workspace, you assign them a name and they are 
bound to that name. If you assign another object to that name, the function will 
be overwritten. If you want to look at any function in R, you can just type in 
it's name to inspect the source. This is most useful for user defined functions 
or functions in add-on packages:

```{r}
dplyr::ntile
```

Functions can be made very specific to your workspace and defined 
within your script, or you can make general functions that are helpful to you and 
store them in an external script that you load when doing certain work:

```{r eval=FALSE}

source("path/to/mySuperCoolFunctions.R")

```


## String Functions

R has a number of standard functions for processing character or string values.

`trimws(" a character ")` will eliminate all whitespace before and after a 
a character value. It can also be used to eliminate whitespace only on a particular 
side `trimws(" a character ", which = "left")` or 
`trimws(" a character ", which = "right")`. 

`toupper` and `tolower` convert all of the characters in a string to either 
upper or lowercase. 

Note that `trimws`, `toupper`, and `tolower` can all be used on either a single 
character value, or a vector of length **n** character values. 

If you need to do string pattern matching and replacement you will use the 
`grep` command. 

The `grep` command returns a numeric vector that indicates the position in the 
character vector that a match is found. This makes subsetting only variables 
that match a certain name easy.

```{r}
# Return only the column positions that match
grep("cyl", names(mtcars))
# use in subset
mtcars[, grep("mpg", names(mtcars))]

```

You can use the `grepl` function to return a vector the same length as the 
character vector filled with TRUE values where a match is found and `FALSE` 
for elements that do not match.

```{r}
grepl("cyl", names(mtcars))
```

For replacement and substitution you will want to try `gsub`.

```{r}
data(mtcars)
row.names(mtcars)

gsub("Maserati", "!!!!MASERATI!!!", row.names(mtcars))

```

With any of these functions if you know regular expressions you can pass more 
complex patterns to be matched. This can be very powerful in processing text 
fields like names, addresses, and phone numbers. 

To find more: `?regex` . You can also look at the excellent `stringr` package 
for even more powerful functions for processing text. 

## Math Functions

R provides a wide array of math functions. 

```
min
max
median
mean
round
```

An important thing to remember when using math functions in R is that the `NA` 
value propogates. If a single `NA` is present in a vector then many mathematical 
operations on that vector will return `NA`. Many mathematical operators include 
an argument `na.rm` that allows you to tell the function to ignore missing values. 

```{r}
mtcars$hp[8] <- NA # add a missing value to demo
mean(mtcars$hp)
mean(mtcars$hp, na.rm=TRUE)
min(mtcars$hp)
max(mtcars$hp)
median(mtcars$hp)
min(mtcars$hp, na.rm=TRUE)
max(mtcars$hp, na.rm=TRUE)
median(mtcars$hp, na.rm=TRUE)
```

R does not include a `mode` function by default (`?mode` brings up something else 
entirely). Users can define their own mode function. In R you will have to define 
your own function based on how you intend to handle ties. One example is the 
`statamode` function in the `eeptools` package. To call a function in a package 
without loading the whole package you can use the shortcut `::`

```{r}
eeptools::statamode(mtcars$cyl)

```

## Statistical Functions

R has a number of convenient functions for statistical transformations of data. 
The `scale` function pulls double duty, it can center data, scale data, or scale 
and center data.

```{r}
data(mtcars)
# regular
summary(mtcars$hp)
# center
summary(scale(mtcars$hp, scale = FALSE))
# scale
summary(scale(mtcars$hp, center = FALSE))
# scale and center
summary(scale(mtcars$hp))
```

This function conveniently stores the centering value (the mean) and the scale 
factor (the standard deviation) as attributes.

```{r}
# scale and center
mtcars$hp_scaled <- scale(mtcars$hp)

# mean
attr(mtcars$hp_scaled, "scaled:center")
# sd
attr(mtcars$hp_scaled, "scaled:scale")
```

The most convenient way to bin data is to use the `ntile` function in the `dplyr` 
package:

```{r}
# Create example data, 25 draws from a random normal distribution
xvar <- rnorm(25)
ntile(xvar, 3)
table(ntile(xvar, 3))
```

## Date Functions

It is highly recommended that you use the free add-on package to R, `lubridate`, 
for processing date and date-time objects in R. This package contains a number 
of simple functions that make date/time operations intuitive and expressive.

```{r}
library(lubridate)
# Convert string to date
coolDay <- mdy("05-25-1986")
# Get month
month(coolDay)
# Get year
year(coolDay)
# Get day
day(coolDay)
# Get week
week(coolDay)
# Day of the week
wday(coolDay, label = TRUE, abbr = FALSE)
```

# 4. Macros

R uses functions and objects instead of macros. 

# 5. Operators

R uses the standard logical operators `>`, `<`, `>=`, `<=`, `==`, `!=` greater 
than, less than, greater than or equal to, less than or equal to, equal to, or 
not equal to for comparison of values. Using these results in returning a 
logical vector, TRUE/FALSE which can be used as an index or tabulated directly.

```{r}
table(mtcars$hp > 300)
summary(mtcars[mtcars$hp > 250, ])
```

`==` and `!=` can be used for comparing character values as well.

```{r}
mtcars$mpg[row.names(mtcars) == "Valiant"]
summary(mtcars$mpg[row.names(mtcars) != "Valiant"])
```

For compounding logical operators R allows you to use `&` for and and `|` for 
or. 

```{r}
mtcars[mtcars$disp > 100 & mtcars$drat < 3,]
mtcars[mtcars$disp > 160 | mtcars$drat < 3.25,]

# You can also nest logical statements

mtcars[(mtcars$disp > 160 | mtcars$drat < 3.25) & 
          mtcars$carb == 3,]
```

Additionally useful is the `%in%` command. This allows you to test whether elements 
in one vector match any of the elements in another vector. 

```{r}
mtcars[row.names(mtcars) %in% grep("Mazda", row.names(mtcars), value=TRUE), ]

```


To test whether missing values are present, R uses the awkward `is.na()` function.

```
table(is.na(mydata$vector1))

```

If you want to filter out NA values from an operation, you need to use `!is.na()`:

```
summary(mydata$vector1[!is.na(mydata$vector1)])

mydata$vector1[!is.na(mydata$vector1)] <- "Not Missing"

```


# 6. Commands

In interactive mode, the way you will most commonly use R, there is not an 
equivalent of assert. However, you use logical tests and ask R to issue an 
error or warning if the tests fail. 

```
mtcars$mpg <- mtcars$mpg *100
stopifnot(min(mtcars$mpg) < 10)
mean(mtcars$mpg)
```

## Printing to the console

Alternatively, and good for debugging, you can have R print arbitrary output to 
the console. 

```

if(myValue > 7){
  print("myValue is now greater than 7")
}

```

# 7. Loops

`for` loops in R are similar to `for` loops in many other languages. They consist 
of an iterator and a vector over which to iterate:

```
for(i in 1:10){
  print(i^2)
}

```

Note that `i` is a variable defined in the global environment. It need not be 
numeric. You can also iterate over any arbitrary vector:

```
for(letter in letters){
  print(letter)
}
```

# 8. The Tidyverse

The "tidyverse" is a name for a collection of R packages that make R code much 
easier to read and easier to remember. The SDP toolkit makes use of a number of 
these packages, most importantly, `dplyr`. 

## The Packages

Installing external libraries contributed by users around the world may cause 
some unease. Will the code be stable? Does it work as expected? These are good 
questions, especially in production level systems where you want the same code 
to do the same tasks unattended for a long period of time. 

In general packages in the `tidyverse` should be considered as meeting the 
requirements for use in production level systems. These are all stable packages 
and any large changes in them will be communicated loudly to users through the 
package update process (if a function will be changed or removed your code will 
start telling you after you update).

Another advantage is that these packages all share a common philosophy to their 
naming conventions, organization of functions, and documentation. This makes it 
easy to anticipate how to use new functions, get help with functions, and more. 

You can learn more by reading about what packages are included in this suite and 
the design philosophy of these packages by typing:

```{r eval=FALSE}
library(tidyverse)
vignette("manifesto")
```


## Pipelines

A key features of the tidyverse is that your code can be made easier to read 
through the use of code "pipes" or "pipelines". This is a new way of writing 
code in R, but is inspired by Unix system utilities and many other programming 
languages. 

Pipelines work by passing data elements along them to specified places in the 
next function call. 

Here is how code might look without a pipeline:

```{r}
data(mtcars)

round(mutate
      (summarize_all(
        group_by(
          filter(mtcars, hp > 100), cyl), 
        .funs=mean), 
        kpl = mpg * 0.4251), 
      digits = 2)
```

This command is read from the inside out which means looking into the call and 
trying to find where it starts (hint: `filter`), then reading the arguments 
outward from there. For complex arguments this is hard. This code can also be 
written in stages:

```{r}

subData <- filter(mtcars, hp > 100)
cylSum <- summarize_all(group_by(subData, cyl), .funs = mean)
cylSum$kpl <- cylSum$mpg * 0.4251
print(round(cylSum, digits = 2))
```

The above is easier to read, but it creates a lot of clutter in the workspace. 
After this series of commands `subData` and `cylSum` objects have been added to 
the working environment in R. An advantage of writing 
functions is that they do not show temporary data objects to the user and they 
clean up after themselves. 

Pipelines provide the tidiness of functions with the readability and 
ease of writing of the code above:

```{r}
car_data <- filter(mtcars, hp > 100) %>% 
  group_by(cyl) %>% 
  summarize_all(.funs = mean) %>%
  mutate(kpl = mpg * 0.4251) %>% 
  round(digits = 2)
car_data

```

Reading the code above is natural. The transformations applied on the left 
hand side of the `%>%` are passed into the first position of the function on the 
right hand side of the `%>%`. You can read the `%>%` operator as the English 
word `THEN`. This becomes a readable block of text that is easy to follow, 
easy to read, and easy to anticipate the expected output. 

There is another convenient `pipe` operator in the `magrittr` package which can 
save your fingers a lot of typing -- the `%<>%` operator. It allows you to replace 
an object on the left hand side with the transformations applied to it on the 
right hand side. Here is how it works:

```{r}
library(magrittr) # %<>% is not automatically loaded with dplyr
data(mtcars)
mtcars$mpg
mtcars$mpg %<>% sqrt
mtcars$mpg
```

Instead of having to type `mtcars$mpg <- sqrt(mtcars$mpg)` simply type 
the object you want to transform, the pipe operator, and the function you want
to apply. 

## Visualizing Data with ggplot2

A key advantage of using R is the data visualization capabilities it has. R 
boasts a number of ways to make statistical graphics and the more you use R 
the more you will come across `base`, `lattice`, and `ggplot2` graphics. It 
is recommended you start with using `ggplot2` because of its consistent language 
and the modular way it allows users to assemble complex graphics from simple parts. 


```{r}
library(ggplot2)
data(mtcars)

ggplot(mtcars, aes(x = hp, y = mpg)) + 
  geom_point() + geom_smooth() + 
  scale_x_continuous(limits = c(40, 360), 
                     breaks = seq(40, 360, 60)) +
  scale_y_continuous(limits = c(10, 40), 
                     breaks = seq(10, 40, 5)) +
  theme_classic()

ggplot(mtcars, aes(x = hp, y = mpg)) + 
  geom_point() + geom_smooth(method = "lm") + 
  scale_x_continuous(limits = c(40, 360), 
                     breaks = seq(40, 360, 60)) +
  scale_y_continuous(limits = c(10, 40), 
                     breaks = seq(10, 40, 5)) +
  theme_classic() + facet_wrap(~vs, labeller = "label_both")

```

A key concept of `ggplot2` is the use of the `+` sign to combine distinct elements 
of a visualization. In the example you can see that the first line of code 
sets up the dataset to be used, `mtcars`, and mapping of the variables in that 
dataset to data elements `x` position and `y` position. 

The next line describes the visual geometries to be mapped to the data, here 
both a `geom_point` for scatterplot points and a `geom_smooth` for a trend 
line through those points. 

The next lines set the scale -- or how the axes are drawn. Here the range of 
the axes is manually specified (though `ggplot2` will try to automatically pick 
good defaults), as are the breaks. 

The final line allows fine-tuned control of the overall look of the elements 
of the graph that are not related to the data (axis lines, tick marks, font 
sizes for labels, etc.). `ggplot2` comes pre-packaged with a number of plot 
themes, and more can be found online. 

You can learn more about `ggplot2` from a number of high quality `ggplot2` 
tutorials online and the excellent package documentation. 


# R Session Information

It is a good idea to include your R Session Information in scripts you run and 
files you output so when you come back to a project you know what version of R 
and the external packages you used it came under. 

```{r}
print(sessionInfo(),locale = FALSE)
```

\vfill
Last updated January 2017.

© 2016 President and Fellows of Harvard College.
